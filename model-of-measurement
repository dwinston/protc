being:reality:universe = the full 4d(?) universe
being-subset = 
s-relevant:s-measured??
s-measured:subset-measured:being-measured:being-subset-measured:s-image
s-prior: -dependent: -protocol: -dependency: -causal
measure:
Ω-measure:omega-measure:total-measure
agent-measure
relevant-measure
symbol:symbolic-rep:symbolic representation
state:
agent:executor
movement:energy dissapation:"causal" state transition:energy-matter-interconversion
existence:the eternal
implementation:representation  ; is this a conflation of two things? ... how to bring something into being exactly vs symbolically maybe?, one deals with existence <-> symbol (representation) the other being <-> symbol ARGH SO HARD TO THINK ABOUT CLEARLY
implement:execute:actualize = take a symbolic-representation of some being-subset and bring it into being
record:instantiate:represent = take a symbolic-representation of some non-being existence and bring it into being
alpha-record:α-record = the record function that when given the exact symbolic representation of a subset of being returns that subset, when given an incomplete (uncertain) symbolic representation of that being it returns bottom, inverse of omega-measure
tau:get-time-interval
invariant:
⊥:bottom:\bot:
->:evaluates-to
protocol = a specification for a being-subset that produces a semi-known being-subset (allows the inference of a subset of the state without actually having to call measure on it) ; find a better way to say this

; predicates
measureable?
implementable?
representable?
state?


(measure being) -> bottom
(record (omega-measure being)) -> being ; some fun comments about the computational complexity of being here...
(measure being-subset) -> symbol ; putting causal bounds on subsets...
(record symbol) -> representation
(= (alpha-record (omega-measure being-subset)) being-subset) -> #t ; subsets are implementations of the symbols of themselves
(record (measure subset)) ; science, ignoring the implementation details
(protocol s-prior) -> s-related inferred-state ; need something more for inferred-state...

measure <-agent-> movement/energy dissipation implies that any subset of the universe that takes in
energy/particle/matter and emits photos at a lower energy could be considered to be an agent (hrm... where
do black holes fit in here... something to do with the timescale of reemission... or maybe the implementation
of a measurement...)

Theorem 1: s-relevant is always bounded IF you are willing to specify a non-zero uncertainty/finite
precision. This holds for universes where the underlying reality has a finite starting point (ie not
doubly infinite in time, or where there is a (space/)time singularity a few years back) and where
there is a speed limit (c) aka where there is locality. In universes without locality this theorem
might still hold if there were something like the 2nd law of thermodynamics that prevented non-locality
from magically popping in and messing with results (a cost function with distance or something). It may
just be a requirement that the causal topology of the underlying reality be connected, meaning that
causes cannot 'jump' from one part of reality to the next without passing through the intervening bits.
Is it even possible to make measurements in non-local universes, probably, which means that simply being
able to transform s-measure into a symbol is not sufficient to satisfy T1. The intuition for this in
local, starting universes is that there is a finite amout of time that can cause events (ignoring the
future causal entanglement stuff for the moment) and the speed limit provides the classic light cone
in space that places a finite limit on the distance from which other parts of the universe can effect
each other. The evidence supporting a variety of R^2 phenomena also provide much stronger constraints
on the bounds for s-relevant. (If information can leak from one universe to the next through the starting
singularity I'm going to be super pissed here.)

Independent of the underlying time constants of being processes (taus, temporal variability) are all
primitive (being) measurements effectively instantaneous? That is, is there any process of measurement
that can produce a number that is the product of multiple points in time. I want to say that there is
root mean squared measurements of AC current seem like the natural candidate, the question is whether
RMS is actually a primitive measure. The transition from being -> existence (symbolization) is most
definitely instantaneous, so in that sense calling (measure being-subset) is instantaneous? Need to
think about this and the implications one way or another.

Is naming a type of high level/complex measurement function (perhaps ill-specified)? Traditional naming
definitely relies on a number of black box measurements (eg visual processing, olfactory processing)
and produces a symbol at the other end. This is not all that different from a number that is produced
by a thermocouple when you have no idea that it is a thermocouple that measures the temperature nor
an inkling about the principles underlying it nor the ability to build one. The only difference would
seem to be the number of black box steps, or more simply the magnitude of our ignorance. This leads
directly to a correlary which is 'what is the error of naming?' If naming is a measure then it MUST
be possible to measure whether it was assigned correctly, either by comparison to a type specimine or
by comparison to an average of all named specimines. I'm going to go with NO it is NOT a measurement
because there is nothing in the underlying being that can be represented by the name that is given.
The name is a function from symbols to symbols NOT from being -> symbols. There is no being measure
that can produce a name.

What is the relationship between black boxes and Ω-measures? Re: The boundary problem when trying to
define closed being-subsets. Imagine a naive being-subset, a 4 dimensional tract of space time bounded
in space using some inertial reference (issues arise) and in time by defined t0 and t1. Now imagine
that inside this black box we wish to study the decay of some radioactive element by scintillation.
Unfortunately for us at some point between t0 and t1 a cosmic ray happens to strike our scitillation
vial and produce a signal which appears to be that of our radioactive element. Even more unfortunately
imagine that that very cosmic ray has as its origin a galaxy 13 billion light years away. That cosmic
ray enters our being-subset some time between t0 and t1 and immediately extends the true t0 back 13
billion years and an (apparently) considerable distance in space. Every single being-subset will face
this or a similar boundary problem. How do we deal with this? [Practically Thresholding. Repeated measures.]
First we need to note that this example shows why I say that our original being subset was naively
defined. Actual being subsets need not be continuous in space time, indeed space time may be an illusion
that hides the underlying causal topology of being (significant issues regarding locality arise here).
Given the practical needs of the working scientist (hah) we need a solution to this problem that neatly
encapsulates such cosmic well wishers so that we can evaluate the match between the reality of the
true being-subset (about which we have already said we would like not to have to specify here) and
the black box we are using to try to approximate that being-subset. This seems to provide a direct
and very satisfactory answer to originally posed question (and could have been determined from the
long standing use of black boxes as tools for thinking and modeling). Black boxes are representations
of a being-subset that are missing the complement of the black box. Every black box must have its
complement represented explicitly in order for the union of the two (BB and BBC) to fully capture
the being-subset. This formalizes the process of science as the process of drawing the boundary
between the black box and the BBC for the purposes of assigning causes to measurements. More fully
in our original black box we had placed a scintillation vial, a photon detector of some sort, and a
radioactive source (we'll do this in a vacuum at STP for simplicity). We asserted that any blip of
energy that we detected on the photon detector must correspond to an emission from the source. In
fact, one of the blips came from a phenomenon outside the black box. There are a couple of ways
that science can deal with this. 1) Try to eliminate interaction between the black box (literal in
this case) and the BBC. 2) Statistics: loose the BBC in the noise. Number one requires an awareness
that the BBC exists and also a belief that it is possible to shield oneself from it (a tour of an
electrophysiology lab will reveal that some practitioners have completely given up on certain kinds
of shielding). Number two requires the ability to make repeated measurements and often requires some
hefty assumptions (eg when collecting data on diverse populations of biological subjects). Note that
the BBC does not include all of being outside the black box, only the part of the being subset not
explicitly enumerated inside the black box. Note further that knowledge BBCs can be developed over
time and shared so that known sources of error can be managed. More note: black boxes do not
initially have to have any known or named phenomenon or mathematical equivalent in them (naming a
thing is essentially black boxing). In the beginning the black box includes only the measurement
equipment that produces the symbolic representation (note that the point of transition from
measurement device to environment is not always entirely clear). Adjusting the boundary, or expanding
the box from the measurement equipment to the phenomena involved is what variation in the protocol
enables, or model building from other types of measurement or guesses enables. Interestingly in these
cases the implementation of the protocol itself (ie the 4d spatiotemporal thing that something
executes as a result of the representation of the protocol artifact) may (can and should?) also
be considered part of the black box (it could also be a known part of the BBC, this version could
be more useful for refinement, but it would necessitate a more thorough illucidation of how things
pass from the truly unknown into the BBC, something that is probably very important to have...).
Either of these representations for the protocol itself have the very nice property of assuring
that the phenomena in question remain stable (as a function of the protocol) and that the only
thing that changes is our ability to name, identify, predict, and control the contents of the black box.

Dealing with BBCs. We probably want nested BCCs maybe up to 1 level, splitting it into known and unknown
sources of 'noise' in our measurements, so as to enable questions about whether the variation in the
experimental protocol is sufficient to explain the variation in our results (and similar). To explore
this let us consider how one might go about identifying 'noise.' Consider the electrophysiologist who
suddenly identifies a signal in the output of his data acquisition amplifier which he is fairly certain
he wants nothing to do with. Ignoring for the time being of how he knows he doesn't want this (experience)
how does he go about removing it? In principle this should be fairly easy [hah] since all the pieces of
equipment are know and he merely needs to twiddle each one of them independently to see if the noise changes.
Let us pretend he accomplishes this because of the tight feedback loop that exists between the visualization
of the numbers coming out of the amplifier and his exceeding ability to twiddle things. What has he done?
In the model of the black box he has identified that the cause of the measurements he is making are not
those that he wants and so immediately creates a new black box containing 'the-source-of-the-noise' and
BBCk that contains all the equipment involved in the experiment and a BBCu (unknown) that contains everything
else. At this point in time the black box itself is empty because the-source-of-the-noise references a
null pointer, and the-source-of-the-noise is either in BBCu and BBCk. This is where we start. From here
(non)systematic twiddling of members of BBCk will either reveal that the-source-of-the-noise (henceforth
merely 'the-source') is contained within a being-subset corresponding to a member (or set of members) of BBCk,
or that the-source is in BBCu at this point the electrophysiologist can either go on the war path and find
the solid state physics graduate students upstairs that are playing with a tesla coil (exhaustive search)
or he can try to shield his rig from the diversions of his underworked and overpaid neighbors. In the first
case we can directly assign the-source to a known phenomenon and can place the offending individuals in
the black box of shame. In the second case the outcome is thus that while we do not know the exact nature
of the-source we have found a kind of complementary knowledge about how we can modify BBCk in such a way
as to shield our original black box from outside interference. [PRACTICAL] This immediately suggests that
we need a way to express that certain parts of a protocol exist to shield our black box from members of
BBCu that we do not have time to fully name, identify, predict, and control. Movement from BBCu to BBCk
and from BBCu to BB or BBCk to BB (retraction watch!) are events that seem to live somewhat outside
the realm of the protocol, but we may want to manage this in some way since it is valuable for the
record keeping of the usually opaque events that lead to the development of a known good protocol.

What is a being measure? What is a computed measure?
Computed measures produce numbers that have been modified acording to (intensional) symolic operations,
even if they are implemented in hardware (eg the output of the gain step of a dsp). In contrast being
measures are the first available symbolically representable state of underlying being. For example the
analog voltage measured across a known resistor PRIOR to digitization so that digitization aliasing
can be accounted for in the provenance. Not entirely sure how to make this kind of distinction practical
in protc or anything else that implements this model of measurement. I don't think there is an issue
with also allowing the digitized signal to be the 'first' symbolic representation since often in science
we might no know that there is a prior phenomenon that underlies what we think we are measuring. [Fun times
trying to document the 'historical' protocols that were used to develop standard measures. I wonder if the
protocols look like fixed points or whether they simply piggy back on local (assued) invariants in the
structure of being? The process needed to FIND the invariants will probably look different depending on
the type of invariant and the types of noise it is subject to. The notion that an invariant exists in
the first place should hopefully bear more similarity. Unfortunately this theory of measurement models
hypotheses as if they spring from the Void itself (they might as well).]


; assumptions
	In the best case we want a theory of measurement to make zero assumptions about the nature of the underlying being [reality] we are trying to measure.
	Unfortunately there are some assumptions that are unavoidable when we impose measurement on being.
	All assumptions we make about the underlying nature of being in this theory are equivalent to the assumption that it is possible to take an omega-measure on being.
	Phrased another way: symbols that represent all of being are contained in the class of existences.
	(contains (omega-measure being) existence) -> #t
; implications of assumptions
	Electrons have positions and momentums at the same time, even if we cannot measure them at the same time.
	Note here that the relationship between knowability within the universe and the true nature of being are quite distinct. ; more considerations about proabalistic 'state' here... (ie that Heisenberg's is a statement about knowability/measureability not about being, see evidence that Cochlea allow humans to perform better than expected on the uncertainty principle imposted by the fourier transform)
	That it is possible to recover the string of random numbers that actually came out of the universe's RNG.

; ramblings
	We can never know whether we have obtained an omega-measure.
	The best we can hope to achieve is a relevant-measure, and
	relevant-measure is always defined by a set of axioms, not by being.
	Essentially, there is no oracle that we can use to check our answers,
	there is no revealed truth that can tell us the nature of being.

	Frank makes a good point about 'aximomatic knowability' vs 'scientific knowability'
	I disagree that being able to infer things about the structure of the universe
	prior to the big bang actually counts as being able to infer things about other universes,
	because it is not clear to me that we can *actually* consider singularities to be outside
	being (as much as I might like to for convenience sake). Certainly it wouldn't meet even
	relaxed	criteria for scientific knowability because there is no way to make a direct
	measurement on more than one member of the class of universes leaving us with the n=1 problem.

; todo
	Is it actually possible to constrain the bounds of s-relevant? Yes, but only as a function of a set of axioms (and perhaps data at some point).
	Measuring the voltage between ground and probe sticking things in between the two produces a symbol which represents an extremely complex 'state' variable, and might even be said to only refer to the state of the digitizer or the state of the transistors in the cpu, which is circular and severely unsatisfactory. In this case there is a mental model that can help constrain the problem: a circuit. However, in many cases, and certainly at the start of any new science, those models do not exist or must be imported (as axioms) from some already existing field of knowledge.
